{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "deep_learning_project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fUA_IsIDqn6g",
        "inZMxKfMqrSB",
        "fa_S1OL57esj",
        "32kVy0LsCSYR",
        "peltgEwq7hJ7",
        "FvrT1Q9gyYjI",
        "-qqDBvF58kPR",
        "btItczBktmKv",
        "ZPavzwfnzM13",
        "A4dEHLGw7jW-",
        "FME6VmW_7ll7",
        "9xq22gz_RYV_",
        "L1NsYgxtzxHd",
        "7bXKKQ0DRA6W",
        "YlWDiA5PRDR2"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUA_IsIDqn6g"
      },
      "source": [
        "#Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI4PS86rwjA1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8lNucnHX96k"
      },
      "source": [
        "!pip install pytorch-lightning \n",
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF95cSwxQhi3"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inZMxKfMqrSB"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krBSbrSR5hnL"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim.lr_scheduler as LR\n",
        "from tqdm.notebook import tqdm \n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa_S1OL57esj"
      },
      "source": [
        "#Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVxwGKozr2L-"
      },
      "source": [
        "class AutoregressiveDataset(Dataset):\n",
        "    '''\n",
        "    dataset for an autoregressive task, the output is the input sequence shifted by 1\n",
        "    '''\n",
        "    def __init__(self, path:str, set_type:str, val_size:int, test_size:int, window_size:int = 7, cols_to_use = ['Open'], col_to_sort:str = 'Date',\n",
        "                 scaler = None, scale_sections:int = 1, scaler_type:str = 'minmax'):\n",
        "        super().__init__()\n",
        "        assert (set_type in {'train', 'val', 'test'})\n",
        "        assert (scaler_type in {'standard', 'minmax'})\n",
        "\n",
        "        #read file\n",
        "        self.path = path\n",
        "        self.window_size = window_size\n",
        "\n",
        "        if self.path == 'sns_flights':\n",
        "            flight_data = sns.load_dataset(\"flights\")\n",
        "            npy_df = flight_data['passengers'].values.astype(float).reshape(-1,1)\n",
        "        else:\n",
        "            pandas_df = pd.read_csv(path).sort_values(by=col_to_sort, ascending=True)\n",
        "            npy_df = np.array(pandas_df[cols_to_use]) \n",
        "\n",
        "        npy_df = npy_df[~np.isnan(npy_df).any(axis=1),:] #remove rows with missing values \n",
        "        assert (not np.isnan(npy_df).any())\n",
        "\n",
        "        #split dataset\n",
        "        tot_len = npy_df.shape[0]\n",
        "        start = 0\n",
        "        if set_type == 'test':\n",
        "            start = tot_len - test_size - window_size\n",
        "        elif set_type == 'val':\n",
        "            start = tot_len - test_size - val_size - 2 * window_size\n",
        "        \n",
        "        end = tot_len\n",
        "        if set_type == 'val':\n",
        "            end = tot_len - test_size - window_size\n",
        "        elif set_type == 'train':\n",
        "            end = tot_len - test_size - val_size - 2 * window_size\n",
        "        \n",
        "        npy_df = npy_df[start : end]\n",
        "        npy_df = self.get_whole_sequence_from_df(npy_df)\n",
        "        self.unscaled_sequence = torch.tensor(npy_df)\n",
        "\n",
        "        #normalization        \n",
        "        if scaler is not None:\n",
        "            self.scaler = scaler\n",
        "            npy_df = scaler.transform(npy_df)\n",
        "        else:\n",
        "            scale_window = int((end - start) / scale_sections)\n",
        "            for i in range(scale_sections):\n",
        "                s = scale_window * i\n",
        "                e = (end - start) if (i == scale_sections-1) else s+scale_window \n",
        "                if scaler_type == 'minmax':\n",
        "                    self.scaler = MinMaxScaler(feature_range=(-1,1))    \n",
        "                elif scaler_type == 'standard':\n",
        "                    self.scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "                else:\n",
        "                    self.scaler = None\n",
        "                    break\n",
        "\n",
        "                self.scaler.fit(npy_df[s:e])\n",
        "                npy_df[s:e] = self.scaler.transform(npy_df[s:e])\n",
        "\n",
        "        #transform in pytorch\n",
        "        self.sequence = torch.tensor(npy_df) #.float()\n",
        "        self.val_size = val_size\n",
        "        self.test_size = test_size\n",
        "        self.cols_to_use = cols_to_use\n",
        "        self.col_to_sort = col_to_sort\n",
        "        self.scale_sections = scale_sections\n",
        "        self.dataset_type = 'AutoregressiveDataset'\n",
        "        self.scaler_type = scaler_type\n",
        "\n",
        "    def get_whole_sequence_from_df(self, npy_df):\n",
        "        return npy_df\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.sequence.shape[0] - self.window_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.sequence[idx : idx + self.window_size]\n",
        "        y = self.sequence[idx + 1 : idx + self.window_size + 1]\n",
        "        return {'x': x, 'y': y}\n",
        "\n",
        "    def unscaled_item(self, idx):\n",
        "        x = self.unscaled_sequence[idx : idx + self.window_size]\n",
        "        y = self.unscaled_sequence[idx + 1 : idx + self.window_size + 1]\n",
        "        return {'x': x, 'y': y}\n",
        "\n",
        "    def num_features(self):\n",
        "        return self.sequence.shape[1]\n",
        "\n",
        "    def get_targets(self):\n",
        "        return self.sequence[self.window_size:]\n",
        "    \n",
        "    def get_real_prices(self):\n",
        "        if self.scaler is None:\n",
        "            return self.get_targets().numpy()\n",
        "        return self.scaler.inverse_transform(self.get_targets().numpy())\n",
        "    \n",
        "    def get_predicted_prices(self, model:pl.LightningModule, teacher = False):\n",
        "        if self.scaler is None:\n",
        "            return self.predict(model, teacher).numpy()\n",
        "        return self.scaler.inverse_transform(self.predict(model, teacher).numpy())\n",
        "    \n",
        "    def is_prediction_different_from_price(self):\n",
        "        return self.scaler is not None\n",
        "\n",
        "    def can_output_prices(self):\n",
        "        return self.cols_to_use\n",
        "\n",
        "    def predict(self, model:pl.LightningModule, teacher = False):\n",
        "        y_pred = torch.Tensor()\n",
        "        y_pred = y_pred.to(model.device)\n",
        "        model.eval()\n",
        "        x = self[0]['x']\n",
        "        with torch.no_grad():\n",
        "            for i in range(len(self)):\n",
        "                if teacher:\n",
        "                    x = self[i]['x']\n",
        "                y_hat = model(x.unsqueeze(dim=0).to(model.device))\n",
        "                y_hat = y_hat[0,-1,:].unsqueeze(dim=0)\n",
        "                x = torch.cat((x, y_hat), dim=0)\n",
        "                x = x[1:]\n",
        "                y_pred = torch.cat((y_pred, y_hat), dim=0)\n",
        "                \n",
        "        return y_pred.cpu()      \n",
        "\n",
        "    def __str__(self):\n",
        "        return f'path:{self.path}, window_size:{self.window_size}, split_perc:{self.split_perc}'\n",
        "    \n",
        "    def log(self, config):\n",
        "        config.dataset_type = self.dataset_type\n",
        "        config.path = self.path\n",
        "        config.window_size = self.window_size\n",
        "        config.val_size = self.val_size\n",
        "        config.test_size = self.test_size\n",
        "        config.cols_to_use = self.cols_to_use \n",
        "        config.col_to_sort = self.col_to_sort \n",
        "        config.scale_sections = self.scale_sections\n",
        "        config.scaler_type = self.scaler_type\n",
        "    \n",
        "    def plotPredictions(self, model, teacher, y_axis_name, plot_name, log, target_idx):\n",
        "        y = self.get_targets()\n",
        "        y_pred = self.predict(model, teacher)\n",
        "        plotPrediction(y, y_pred, y_axis_name, plot_name, log=log, idx_target=target_idx)\n",
        "        return y, y_pred\n",
        "    \n",
        "    def feature_name(self, i):\n",
        "        return self.cols_to_use[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lwaS0udWgYt"
      },
      "source": [
        "class AutoregressiveDistanceDataset(AutoregressiveDataset):\n",
        "    '''\n",
        "    dataset[i] = sequence[i+1] - sequence[i]: we predict difference in prices not prices.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, path:str, set_type:str, val_size:int, test_size:int, window_size:int = 7, cols_to_use = ['Open'], col_to_sort:str = 'Date',\n",
        "                 scaler:MinMaxScaler = None, scale_sections:int = 1, scaler_type:str = 'minmax'):\n",
        "        super().__init__(path, set_type, val_size, test_size, window_size, cols_to_use, col_to_sort, scaler, scale_sections, scaler_type)\n",
        "        self.dataset_type = 'AutoregressiveDistanceDataset'\n",
        "\n",
        "    def get_whole_sequence_from_df(self, npy_df):\n",
        "        self.start_value = torch.tensor(npy_df[self.window_size])\n",
        "        return npy_df[1:] - npy_df[:-1]\n",
        "    \n",
        "    def prices_from_differences(self, differences:torch.Tensor, start:torch.Tensor):\n",
        "        '''\n",
        "        differences has shape Len x NumFeatures\n",
        "        start has shape NumFeatures (the starting values)\n",
        "        '''\n",
        "        ret = start.unsqueeze(-1)\n",
        "        actual = start.clone().detach()\n",
        "        for i in range(differences.shape[0]):\n",
        "            actual += differences[i]\n",
        "            ret = torch.cat((ret, actual.unsqueeze(-1)))\n",
        "        return ret\n",
        "\n",
        "    def get_real_prices(self):\n",
        "        return self.prices_from_differences(torch.tensor(self.scaler.inverse_transform(self.get_targets().numpy())), self.start_value).numpy()\n",
        "    \n",
        "    def get_predicted_prices(self, model:pl.LightningModule, teacher = False):\n",
        "        return self.prices_from_differences(torch.tensor(self.scaler.inverse_transform(self.predict(model, teacher).numpy())), self.start_value).numpy()\n",
        "    \n",
        "    def is_prediction_different_from_price(self):\n",
        "        return True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32kVy0LsCSYR"
      },
      "source": [
        "#Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2WEaJw4CT11"
      },
      "source": [
        "def expanded_trend_loss4(y, y_pred):\n",
        "    '''\n",
        "    this is the loss described in our report, the others are for testing\n",
        "    '''\n",
        "    real_diff = torch.exp(y[:, 1:] - y[:, :-1])\n",
        "    pred_diff = torch.exp(y_pred[:, 1:] - y_pred[:, :-1])\n",
        "    tot_diff = torch.tanh(real_diff - pred_diff)\n",
        "    return torch.mean(tot_diff * tot_diff)\n",
        "\n",
        "def trend_loss(y, y_pred):\n",
        "    real_diff = y[:, 1:] - y[:, :-1]\n",
        "    pred_diff = y_pred[:, 1:] - y_pred[:, :-1]\n",
        "    tot_diff = real_diff - pred_diff\n",
        "    return torch.mean(tot_diff * tot_diff)\n",
        "\n",
        "def expanded_trend_loss(y, y_pred):\n",
        "    real_diff = torch.exp(y[:, 1:] - y[:, :-1])\n",
        "    pred_diff = torch.exp(y_pred[:, 1:] - y_pred[:, :-1])\n",
        "    tot_diff = real_diff - pred_diff\n",
        "    return torch.mean(tot_diff * tot_diff)\n",
        "\n",
        "def expanded_trend_loss2(y, y_pred):\n",
        "    real_diff = y[:, 1:] - y[:, :-1]\n",
        "    pred_diff = y_pred[:, 1:] - y_pred[:, :-1]\n",
        "    tot_diff = torch.sigmoid(real_diff - pred_diff) - 0.5\n",
        "    return torch.mean(tot_diff * tot_diff)\n",
        "\n",
        "def expanded_trend_loss3(y, y_pred):\n",
        "    real_diff = torch.exp(y[:, 1:] - y[:, :-1])\n",
        "    pred_diff = torch.exp(y_pred[:, 1:] - y_pred[:, :-1])\n",
        "    tot_diff = torch.sigmoid(real_diff - pred_diff) - 0.5\n",
        "    return torch.mean(tot_diff * tot_diff)\n",
        "\n",
        "def apply_loss(loss_name, y, y_pred, params={}):\n",
        "    if loss_name == 'mse':\n",
        "        return F.mse_loss(y, y_pred)\n",
        "    if loss_name == 'trend':\n",
        "        return trend_loss(y, y_pred)\n",
        "    if loss_name == 'exp_trend':\n",
        "        return expanded_trend_loss(y, y_pred)\n",
        "    if loss_name == 'exp_trend2':\n",
        "        return expanded_trend_loss2(y, y_pred)\n",
        "    if loss_name == 'exp_trend3':\n",
        "        return expanded_trend_loss3(y, y_pred)\n",
        "    if loss_name == 'exp_trend4':\n",
        "        return expanded_trend_loss4(y, y_pred)\n",
        "    if loss_name == 'mse+trend':\n",
        "        return F.mse_loss(y, y_pred) + params.get('lambda', 1.0) * trend_loss(y, y_pred)\n",
        "    if loss_name == 'mse+exp_trend3':\n",
        "        return F.mse_loss(y, y_pred) + params.get('lambda', 1.0) * expanded_trend_loss3(y, y_pred)\n",
        "    if loss_name == 'mse+exp_trend4':\n",
        "        return F.mse_loss(y, y_pred) + params.get('lambda', 1.0) * expanded_trend_loss4(y, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peltgEwq7hJ7"
      },
      "source": [
        "#Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL6MxRyGuckr"
      },
      "source": [
        "class GeneralModel(pl.LightningModule):\n",
        "    '''class to implement some lightning methods common to all models'''\n",
        "    def __init__(self, loss_name, learning_rate, use_lr_sched, lr_decay_step, lr_decay_gamma):\n",
        "        super().__init__()\n",
        "        self.loss_lambda = 1\n",
        "        self.loss_name = loss_name\n",
        "        self.initial_learning_rate = learning_rate\n",
        "        self.use_lr_sched = use_lr_sched\n",
        "        self.lr_decay_step = lr_decay_step\n",
        "        self.lr_decay_gamma = lr_decay_gamma\n",
        "        self.target_for_current_step = None\n",
        "\n",
        "    def step(self, batch):\n",
        "        x = batch['x']\n",
        "        y = batch['y']\n",
        "        self.target_for_current_step = y #some models might need y for the forward\n",
        "        y_hat = self.forward(x)\n",
        "        return apply_loss(self.loss_name, y, y_hat, {'lambda': self.loss_lambda})\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self.step(batch)\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self.step(batch)\n",
        "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch, batch_idx, dataset_idx = 0):\n",
        "        loss = self.step(batch)\n",
        "        self.log('test_loss', loss, logger=True)\n",
        "        return loss\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.initial_learning_rate)\n",
        "        if self.use_lr_sched:\n",
        "            lr_sched = LR.StepLR(optimizer, step_size=self.lr_decay_step, gamma=self.lr_decay_gamma)\n",
        "            return {'optimizer': optimizer, 'lr_scheduler': lr_sched}\n",
        "        return optimizer\n",
        "\n",
        "    def number_of_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    #def training_epoch_end(self, training_step_outputs):\n",
        "    #    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvrT1Q9gyYjI"
      },
      "source": [
        "##MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dfsg6wwya5G"
      },
      "source": [
        "class MLP(GeneralModel):\n",
        "    '''MLP shared across all sequence'''\n",
        "    \n",
        "    def __init__(self, input_size, output_size, hidden_size = 100, num_hidden_layers = 1, use_lr_sched = True, learning_rate = 1e-3, lr_decay_step = 50, lr_decay_gamma = 0.1, loss_name:str = 'mse'):\n",
        "        super().__init__(loss_name, learning_rate, use_lr_sched, lr_decay_step, lr_decay_gamma)\n",
        "        self.inputlayer = nn.Linear(input_size, hidden_size)\n",
        "        self.hiddenlayers = nn.ModuleList([])\n",
        "        for i in range(num_hidden_layers):\n",
        "            self.hiddenlayers.append(nn.Linear(hidden_size, hidden_size))\n",
        "        self.outputlayer = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, x): #used for inference\n",
        "        x = self.inputlayer(x)\n",
        "        for hl in self.hiddenlayers:\n",
        "            x = F.relu(hl(x))\n",
        "        return self.outputlayer(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qqDBvF58kPR"
      },
      "source": [
        "##LSTM based"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8ehkEp6zp_I"
      },
      "source": [
        "class StackedLSTM(GeneralModel):\n",
        "    '''standard LSTM model (the first model included in the report) '''\n",
        "    \n",
        "    def __init__(self, input_size, output_size, hidden_size = 100, num_layers = 1, dropout_prob = 0.0, feed_forward_depth = 1,\n",
        "                 use_lr_sched = True, learning_rate = 1e-3, lr_decay_step = 50, lr_decay_gamma = 0.1, loss_name:str = 'mse', loss_lambda:float = 1.0):\n",
        "        super().__init__(loss_name, learning_rate, use_lr_sched, lr_decay_step, lr_decay_gamma)\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout_prob, batch_first=True)\n",
        "        self.fc = nn.ModuleList([])\n",
        "        for i in range(feed_forward_depth - 1):\n",
        "            self.fc.append(nn.Linear(hidden_size, hidden_size))\n",
        "        self.fc.append(nn.Linear(hidden_size, output_size))\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.loss_lambda = loss_lambda\n",
        "\n",
        "    def forward(self, x): #used for inference\n",
        "        hiddens = (torch.zeros((self.num_layers, x.shape[0], self.hidden_size)).to(self.device),\n",
        "                    torch.zeros((self.num_layers, x.shape[0], self.hidden_size)).to(self.device))\n",
        "        x, hiddens = self.lstm(x, hiddens)\n",
        "        for i in range(len(self.fc)-1):\n",
        "            x = F.relu(self.fc[i](x))\n",
        "        x = self.fc[-1](x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQsA3r1ctrnf"
      },
      "source": [
        "class EncoderDecoderLSTM(GeneralModel):\n",
        "    '''LSTM with encoder/decoder paradigm (performances are not much different from standard lstm)'''\n",
        "    def __init__(self, input_size, output_size, hidden_size = 100, num_layers = 1, dropout_prob = 0.0, use_lr_sched = True, learning_rate = 1e-3, lr_decay_step = 50, lr_decay_gamma = 0.1,\n",
        "                 loss_name:str = 'mse', loss_lambda:float = 1.0, decoder_input = 'original_input'):\n",
        "        super().__init__(loss_name, learning_rate, use_lr_sched, lr_decay_step, lr_decay_gamma)\n",
        "        assert (decoder_input in [None, 'original_input', 'encoder_output'])\n",
        "        self.encoder = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout_prob, batch_first=True)\n",
        "        dec_in = input_size\n",
        "        if decoder_input == 'encoder_output':\n",
        "            dec_in = hidden_size\n",
        "        self.decoder = nn.LSTM(input_size=dec_in, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout_prob, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.loss_lambda = loss_lambda\n",
        "        self.decoder_input = decoder_input\n",
        "\n",
        "    def forward(self, x): #used for inference\n",
        "        hiddens = (torch.zeros((self.num_layers, x.shape[0], self.hidden_size)).to(self.device),\n",
        "                    torch.zeros((self.num_layers, x.shape[0], self.hidden_size)).to(self.device))\n",
        "        out, hiddens = self.encoder(x, hiddens) #encode the sequence in the hidden state\n",
        "        dec_in = x\n",
        "        if self.decoder_input is None: #feed zeros if we don't want to use inputs in the decoder\n",
        "            dec_in = torch.zeros(x.shape).to(self.device)\n",
        "        elif self.decoder_input == 'encoder_output':\n",
        "            dec_in = out\n",
        "        z, hiddens = self.decoder(dec_in, hiddens) #decode using the original input x and the learned representation of the sequence\n",
        "        z = self.out(z) #final shared dense layer\n",
        "        return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btItczBktmKv"
      },
      "source": [
        "##Attention-based"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B5WWffTOlLQ"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    '''positional encoding described in https://arxiv.org/abs/1706.03762'''\n",
        "    def __init__(self, d_model, dropout, max_len):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        position = torch.arange(max_len)\n",
        "        w = 1.0 / (torch.pow(1000, torch.arange(0, d_model, 2) / d_model))\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(torch.einsum('i,j->ij', position, w))\n",
        "        pe[:, 1::2] = torch.cos(torch.einsum('i,j->ij', position, w))\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x * math.sqrt(x.size(2))\n",
        "        x = x + self.pe[:x.size(1)]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-IrwQ_zwCit"
      },
      "source": [
        "class WarmupScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
        "    '''warmup scheduler defined in https://arxiv.org/abs/1706.03762 note that this has to be called after each step, not each epoch'''\n",
        "    def __init__(self, optimizer, warmup_steps = 400, d_model = 128, last_epoch=-1, verbose=False):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.num_steps = 0\n",
        "        self.d_model = d_model\n",
        "        super(WarmupScheduler, self).__init__(optimizer, last_epoch, verbose)\n",
        "        \n",
        "    def get_lr(self):\n",
        "        if not self._get_lr_called_within_step:\n",
        "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
        "                          \"please use `get_last_lr()`.\")\n",
        "\n",
        "        self.num_steps += 1\n",
        "        lr = 1 / math.sqrt(self.d_model) * min(1 / math.sqrt(self.num_steps), self.num_steps * (self.warmup_steps ** (-1.5)))\n",
        "        return [lr] * len(self.optimizer.param_groups)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGOSdbfJFLgX"
      },
      "source": [
        "class Transformer(GeneralModel):\n",
        "    '''transformer model (the second model included in the report: we use lstm to create embeddings and append the input to the encoding before decoding)'''\n",
        "    def __init__(self, input_size, output_size, max_len, embedding_size = 100, dropout_prob = 0.1, warmup_steps = 400, use_lstm_for_embeddings = False, append_input_decoding = True,\n",
        "                 loss_name:str = 'mse', loss_lambda:float = 1.0, attention_heads = 1, num_encoder_blocks = 6, transformer_hidden_size = 2048, \n",
        "                 l2_penalty_lambda = 0.0, use_amsgrad=False, use_lr_sched = True, learning_rate = 1e-3):\n",
        "        super().__init__(loss_name, learning_rate, use_lr_sched, 0.0, 0.0)\n",
        "        \n",
        "        self.seq_len = max_len\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_features_transformer = embedding_size\n",
        "        self.use_lstm = use_lstm_for_embeddings\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "        if not self.use_lstm:\n",
        "            self.embedding = nn.Linear(input_size, embedding_size, bias=False)\n",
        "            self.pos_encoding = PositionalEncoding(self.num_features_transformer, dropout_prob, max_len)\n",
        "        else:\n",
        "            self.lstm = nn.LSTM(input_size=input_size, hidden_size=embedding_size, num_layers=1, dropout=0.0, batch_first=True)\n",
        "        \n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.num_features_transformer, nhead=attention_heads, dropout=dropout_prob, activation='relu', dim_feedforward=transformer_hidden_size, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_blocks)\n",
        "\n",
        "        self.decoder_size = self.num_features_transformer\n",
        "        if append_input_decoding:\n",
        "            self.decoder_size += self.embedding_size\n",
        "        self.append_input_decoding = append_input_decoding\n",
        "\n",
        "        self.out = nn.Linear(self.decoder_size, output_size)\n",
        "        self.init_parameters()\n",
        "        self.loss_lambda = loss_lambda\n",
        "\n",
        "        self.automatic_optimization = False #we do training step by hand because we need to call the scheduler after each step, not epoch\n",
        "        self.l2_penalty_lambda = l2_penalty_lambda\n",
        "        self.use_amsgrad = use_amsgrad\n",
        "\n",
        "    def forward(self, x): #used for inference\n",
        "        if self.use_lstm:\n",
        "            hiddens = (torch.zeros((1, x.shape[0], self.embedding_size)).to(self.device),\n",
        "                    torch.zeros((1, x.shape[0], self.embedding_size)).to(self.device))\n",
        "            x, _ = self.lstm(x, hiddens)\n",
        "            x = self.dropout(F.selu(x))\n",
        "        else:\n",
        "            x = self.dropout(F.selu(self.embedding(x)))\n",
        "            x = self.pos_encoding(x)\n",
        "        \n",
        "        x_enc = x #save encoding of x\n",
        "        \n",
        "        mask = self.get_causal_mask(self.seq_len)\n",
        "        x = self.transformer_encoder(x, mask=mask)\n",
        "        \n",
        "        if self.append_input_decoding:\n",
        "            x = torch.cat((x, x_enc), dim=-1) #concatenate embedding of x with its original encoding\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "    \n",
        "    def get_causal_mask(self, seq_len): #mask to make the layers causal\n",
        "        bmask = (torch.arange(seq_len).unsqueeze(0) <= torch.arange(seq_len).unsqueeze(1)).to(self.device)\n",
        "        smask = torch.zeros((seq_len, seq_len)).to(self.device)\n",
        "        smask[~bmask] = float('-inf')\n",
        "        return smask\n",
        "\n",
        "    def init_parameters(self): #init parameters with xavier\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.initial_learning_rate, betas=(0.9, 0.98), eps=1e-9, weight_decay=self.l2_penalty_lambda, amsgrad=self.use_amsgrad)\n",
        "        if self.use_lr_sched:\n",
        "            lr_sched = WarmupScheduler(optimizer, warmup_steps = self.warmup_steps, d_model = self.num_features_transformer)\n",
        "            return {'optimizer': optimizer, 'lr_scheduler': lr_sched}\n",
        "        return optimizer\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        opt = self.optimizers()\n",
        "        opt.zero_grad()\n",
        "        loss = self.step(batch)\n",
        "        self.manual_backward(loss)\n",
        "        opt.step()\n",
        "        sch = self.lr_schedulers()\n",
        "        if sch is not None:\n",
        "            sch.step()\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        if sch is not None:\n",
        "            self.log('learning_rate', sch.get_last_lr()[0], on_step=True, prog_bar=True, logger=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPavzwfnzM13"
      },
      "source": [
        "##EMA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjwZqYdh7QNm"
      },
      "source": [
        "class ExponentialMovingAverage():\n",
        "    '''exponential moving average model (just used for some quick test) '''\n",
        "    def __init__(self, alpha:float = 0.8):\n",
        "        assert (0 <= alpha <= 1)\n",
        "        self.alpha = alpha\n",
        "        self.coeff = None\n",
        "    \n",
        "    def forward(self, x): #expect x.shape = [batch_size, seq_len, num_features]\n",
        "        if self.coeff is None:\n",
        "            seq_len = x.shape[1]\n",
        "            num_features = x.shape[2] #also number of outputs\n",
        "            self.coeff = torch.full((seq_len,num_features), 1-self.alpha)\n",
        "            exps = torch.unsqueeze(torch.arange(start=seq_len-1, end=-1, step=-1), dim=1)\n",
        "            self.coeff = self.coeff ** exps\n",
        "            self.coeff = self.coeff * self.alpha\n",
        "        return torch.sum(x * self.coeff, dim=1)\n",
        "    \n",
        "    def getFullPredictions(self, test_loader:DataLoader):\n",
        "        y_pred = torch.Tensor()\n",
        "        for batch in test_loader:\n",
        "            y_pred = torch.cat((y_pred, self.forward(batch['x'])))\n",
        "        return y_pred\n",
        "\n",
        "    def searchBestAlpha(self, alphasToTry, loaderVal:DataLoader, target):\n",
        "        bestAlpha = alphasToTry[0]\n",
        "        bestError = -1\n",
        "        for a in alphasToTry:\n",
        "            self.alpha = a\n",
        "            self.coeff = None\n",
        "            y_pred = self.getFullPredictions(loaderVal)\n",
        "            error = F.mse_loss(target, y_pred).item()\n",
        "            if bestError < 0 or error < bestError:\n",
        "                bestAlpha = a\n",
        "                bestError = error\n",
        "        self.alpha = bestAlpha\n",
        "        return self.alpha\n",
        "    \n",
        "    def number_of_parameters(self):\n",
        "        return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4dEHLGw7jW-"
      },
      "source": [
        "#Trading Strategies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Emvjxj2jGhw"
      },
      "source": [
        "def nextday_trading_strategy(y_real, y_pred, price_idx = 0, threshold = 0):\n",
        "    '''trading strategy described in the report'''\n",
        "    if len(list(y_real.shape)) == 2:\n",
        "        y_real = y_real[:, price_idx]\n",
        "    if len(list(y_pred.shape)) == 2:\n",
        "        y_pred = y_pred[:, price_idx]\n",
        "    \n",
        "    assert (y_real.shape == y_pred.shape)\n",
        "    gain_list = []\n",
        "    for i in range(len(y_real) - 1):\n",
        "        prev = 0 if len(gain_list) == 0 else gain_list[-1]\n",
        "        if y_pred[i+1] - y_pred[i] > threshold: #buy at i, sell at i+1\n",
        "            gain_list += [prev + y_real[i+1] - y_real[i]]\n",
        "        else:\n",
        "            gain_list += [prev]\n",
        "    return np.array(gain_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FME6VmW_7ll7"
      },
      "source": [
        "#Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT7k6MFJAzJE"
      },
      "source": [
        "def plotPrediction(y, y_pred, label_name, plot_name, log=True, idx_target=0, real_label='real', predicted_label='predicted'):\n",
        "    '''plots the prediction of a model'''\n",
        "    try:\n",
        "        y = y.numpy()\n",
        "        y_pred = y_pred.numpy()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    plt.clf()\n",
        "    fig, ax = plt.subplots(figsize=(16, 9))\n",
        "\n",
        "    shape = list(y.shape)\n",
        "    if len(shape) == 2:\n",
        "        y = y[:, idx_target]\n",
        "        y_pred = y_pred[:, idx_target]\n",
        "\n",
        "    x = np.arange(y.shape[0])\n",
        "    ax.plot(x, y, label=real_label)\n",
        "    ax.plot(x, y_pred, label=predicted_label)\n",
        "    plt.title(plot_name)\n",
        "    plt.xlabel('time')\n",
        "    plt.ylabel(label_name)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    if log:\n",
        "        wandb.log({plot_name: ax})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxRSw6RDDNYk"
      },
      "source": [
        "def getDataset(name:str, path:str, set_type:str, val_size:int, test_size:int, window_size:int = 7, cols_to_use = ['Open'], col_to_sort:str = 'Date', scaler = None, scale_sections:int = 5, scaler_type:str = 'minmax'):\n",
        "    if name == 'AutoregressiveDataset':\n",
        "        return AutoregressiveDataset(path, set_type, val_size, test_size, window_size, cols_to_use, col_to_sort, scaler, scale_sections, scaler_type)\n",
        "    if name == 'AutoregressiveDistanceDataset':\n",
        "        return AutoregressiveDistanceDataset(path, set_type, val_size, test_size, window_size, cols_to_use, col_to_sort, scaler, scale_sections, scaler_type)\n",
        "\n",
        "def getModel(dataset_train, model_type:str, model_params:dict):\n",
        "    if model_type == 'StackedLSTM':\n",
        "        return StackedLSTM(input_size = dataset_train.num_features(), output_size = dataset_train.num_features(), **model_params)\n",
        "    if model_type == 'Transformer':\n",
        "        return Transformer(input_size = dataset_train.num_features(), output_size = dataset_train.num_features(), max_len = dataset_train.window_size, **model_params)\n",
        "    if model_type == 'EncoderDecoderLSTM':\n",
        "        return EncoderDecoderLSTM(input_size = dataset_train.num_features(), output_size = dataset_train.num_features(), **model_params)\n",
        "    if model_type == 'MLP':\n",
        "        return MLP(input_size = dataset_train.num_features(), output_size = dataset_train.num_features(), **model_params)\n",
        "\n",
        "def plotConfusionMatrix(confusion_matrix, plot_name=None):\n",
        "    '''\n",
        "    plots the confusion matrix\n",
        "    '''\n",
        "    cm = confusion_matrix / np.array([np.sum(confusion_matrix, axis=1)]).T #normalize rows\n",
        "    text = np.array([[\"count:{0}\\n{1:.2f}%\".format(confusion_matrix[i,j], cm[i,j]*100) for j in range(2)] for i in range(2)])\n",
        "    plt.clf()\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    sns.heatmap(cm, annot=text, fmt='', ax=ax, cmap='viridis', square=True, annot_kws={\"size\":12})\n",
        "    plt.ylabel(\"true label\") \n",
        "    plt.xlabel(\"predicted label\")\n",
        "    if plot_name is not None: \n",
        "        plt.title(plot_name)\n",
        "    plt.show()\n",
        "\n",
        "def accuracy_fromcm(confusion_matrix):\n",
        "    return (confusion_matrix[1,1] + confusion_matrix[0,0]) / np.sum(confusion_matrix)\n",
        "\n",
        "def precision_fromcm(confusion_matrix):\n",
        "    return confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[0,1])  #tp / (tp + fp)\n",
        "\n",
        "def recall_fromcm(confusion_matrix):\n",
        "    return confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[1,0]) #tp / (tp + fn)\n",
        "\n",
        "def f1score_fromcm(confusion_matrix):\n",
        "    p = precision_fromcm(confusion_matrix)\n",
        "    r = recall_fromcm(confusion_matrix)\n",
        "    return 2 * p * r / (p + r)\n",
        "\n",
        "def mean_absolute_percentage_error(y, y_hat, eps=1e-8):\n",
        "    return np.mean(np.abs((y - y_hat) / (y + eps)))\n",
        "\n",
        "def confusion_matrix_from_predictions(y_true, y_pred, target_idx = 0, threshold = 0):\n",
        "    '''uses the prediction to build a confusion matrix, where we predict 1 if the model predict to buy and 0 otherwise'''\n",
        "    y_true_class = torch.zeros(y_true.shape[0] - 1)\n",
        "    y_true_class[y_true[1:,target_idx] - y_true[:-1,target_idx] > threshold] = 1\n",
        "\n",
        "    y_pred_class = torch.zeros(y_pred.shape[0] - 1)\n",
        "    y_pred_class[y_pred[1:,target_idx] - y_pred[:-1,target_idx] > threshold] = 1\n",
        "\n",
        "    tp = torch.sum(torch.logical_and(y_pred_class == y_true_class, y_true_class == 1).long())\n",
        "    tn = torch.sum(torch.logical_and(y_pred_class == y_true_class, y_true_class == 0).long())\n",
        "    fp = torch.sum(torch.logical_and(y_pred_class != y_true_class, y_true_class == 0).long())\n",
        "    fn = torch.sum(torch.logical_and(y_pred_class != y_true_class, y_true_class == 1).long())\n",
        "    return np.array([[tn, fp], [fn, tp]])\n",
        "\n",
        "def computeMetric(y_test_true, y_test_hat, y_val_true, y_val_hat, y_train_true, y_train_hat, metric, metric_name, debug, log):\n",
        "    '''computes the metric on test/val/train set and log the results'''\n",
        "    test = metric(y_test_true, y_test_hat)\n",
        "    val = metric(y_val_true, y_val_hat)\n",
        "    train = metric(y_train_true, y_train_hat)\n",
        "    if debug:\n",
        "        print(f'{metric_name} (test):', test)\n",
        "        print(f'{metric_name} (val):', val)\n",
        "        print(f'{metric_name} (train):', train)\n",
        "    if log:\n",
        "        wandb.run.summary[f\"{metric_name}_test\"] = test\n",
        "        wandb.run.summary[f\"{metric_name}_val\"] = val\n",
        "        wandb.run.summary[f\"{metric_name}_train\"] = train\n",
        "\n",
        "def performValidation(trainer, model, dataset_train, dataset_val, dataset_test, loader_test, debug, logging, plotsWithoutTeacher, nextday_trading_price):\n",
        "    '''\n",
        "    function to perform a full validation of the model\n",
        "    '''\n",
        "    trainer.test() #load best checkpoint\n",
        "    trainer.test(dataloaders=loader_test)\n",
        "    \n",
        "    num_feat = dataset_train.get_targets().shape[1]\n",
        "\n",
        "    if plotsWithoutTeacher:\n",
        "        for i in range(num_feat):\n",
        "            y_test, y_test_pred = dataset_test.plotPredictions(model, False, dataset_test.feature_name(i), f'test_predictions_{dataset_test.feature_name(i)}', logging, i)\n",
        "            y_val, y_val_pred = dataset_val.plotPredictions(model, False, dataset_val.feature_name(i), f'val_predictions_{dataset_val.feature_name(i)}', logging, i)\n",
        "            y_train, y_train_pred = dataset_train.plotPredictions(model, False, dataset_train.feature_name(i), f'train_predictions_{dataset_train.feature_name(i)}', logging, i)\n",
        "\n",
        "    for i in range(num_feat):\n",
        "        y_test, y_test_pred_teach = dataset_test.plotPredictions(model, True, dataset_test.feature_name(i), f'test_teacher_predictions_{dataset_test.feature_name(i)}', logging, i)\n",
        "        y_val, y_val_pred_teach = dataset_val.plotPredictions(model, True, dataset_val.feature_name(i), f'val_teacher_predictions_{dataset_val.feature_name(i)}', logging, i)\n",
        "        y_train, y_train_pred_teach = dataset_train.plotPredictions(model, True, dataset_train.feature_name(i), f'train_teacher_predictions_{dataset_train.feature_name(i)}', logging, i)\n",
        "\n",
        "    test_prices = dataset_test.get_real_prices()\n",
        "    test_pred_prices = dataset_test.get_predicted_prices(model, True)\n",
        "    val_prices = dataset_val.get_real_prices()\n",
        "    val_pred_prices = dataset_val.get_predicted_prices(model, True)\n",
        "    train_prices = dataset_train.get_real_prices()\n",
        "    train_pred_prices = dataset_train.get_predicted_prices(model, True)\n",
        "    if dataset_train.is_prediction_different_from_price(): #the dataset predicts something different from the actual price: now we plot the actual price using the predictions\n",
        "        prices = dataset_train.can_output_prices()\n",
        "        for i in range(len(prices)):\n",
        "            plotPrediction(test_prices, test_pred_prices, prices[i], f'test_teacher_prices_{prices[i]}', log=logging, idx_target=i)\n",
        "            plotPrediction(val_prices, val_pred_prices, prices[i], f'val_teacher_prices_{prices[i]}', log=logging, idx_target=i)\n",
        "            plotPrediction(train_prices, train_pred_prices, prices[i], f'train_teacher_prices_{prices[i]}', log=logging, idx_target=i)\n",
        "\n",
        "    #use trading strategies\n",
        "    if nextday_trading_price >= 0:\n",
        "        gain_test = nextday_trading_strategy(test_prices, test_pred_prices, price_idx = nextday_trading_price)\n",
        "        gain_test_optimal = nextday_trading_strategy(test_prices, test_prices, price_idx = nextday_trading_price)\n",
        "        gain_val = nextday_trading_strategy(val_prices, val_pred_prices, price_idx = nextday_trading_price)\n",
        "        gain_val_optimal = nextday_trading_strategy(val_prices, val_prices, price_idx = nextday_trading_price)\n",
        "        gain_train = nextday_trading_strategy(train_prices, train_pred_prices, price_idx = nextday_trading_price)\n",
        "        gain_train_optimal = nextday_trading_strategy(train_prices, train_prices, price_idx = nextday_trading_price)\n",
        "        \n",
        "        plotPrediction(gain_test_optimal, gain_test, 'next-day-gain', 'test next-day-gain', log=logging, real_label='optimal', predicted_label='using model')\n",
        "        plotPrediction(gain_val_optimal, gain_val, 'next-day-gain', 'val next-day-gain', log=logging, real_label='optimal', predicted_label='using model')\n",
        "        plotPrediction(gain_train_optimal, gain_train, 'next-day-gain', 'train next-day-gain', log=logging, real_label='optimal', predicted_label='using model')\n",
        "    \n",
        "    #log summaries\n",
        "    \n",
        "    computeMetric(y_test, y_test_pred_teach, y_val, y_val_pred_teach, y_train, y_train_pred_teach, r2_score, 'r2_score', debug, logging) #r2 score\n",
        "    computeMetric(y_test, y_test_pred_teach, y_val, y_val_pred_teach, y_train, y_train_pred_teach, mean_absolute_error, 'MAE', debug, logging) #MAE\n",
        "    computeMetric(y_test, y_test_pred_teach, y_val, y_val_pred_teach, y_train, y_train_pred_teach, lambda y, y_hat: np.sqrt(mean_squared_error(y, y_hat)), 'RMSE', debug, logging) #RMSE\n",
        "    computeMetric(test_prices, test_pred_prices, val_prices, val_pred_prices, train_prices, train_pred_prices, r2_score, 'r2_score_on_prices', debug, logging) #r2 score on prices\n",
        "    try:\n",
        "        computeMetric(test_prices, test_pred_prices, val_prices, val_pred_prices, train_prices, train_pred_prices, mean_absolute_percentage_error, 'MAPE_on_prices', debug, logging) #MAPE on prices\n",
        "    except:\n",
        "        if debug:\n",
        "            print('mape not computed because of negative values')\n",
        "\n",
        "    if nextday_trading_price >= 0: #evaluate the model as if it was a classifier (of course it will perform worse than an actual classifier, but it's interesting to look at these)\n",
        "        cm_test = confusion_matrix_from_predictions(y_test, y_test_pred_teach)\n",
        "        cm_val = confusion_matrix_from_predictions(y_val, y_val_pred_teach)\n",
        "        cm_train = confusion_matrix_from_predictions(y_train, y_train_pred_teach)\n",
        "        if debug:\n",
        "            plotConfusionMatrix(cm_test, plot_name='confusion matrix test')\n",
        "            plotConfusionMatrix(cm_val, plot_name='confusion matrix val')\n",
        "            plotConfusionMatrix(cm_train, plot_name='confusion matrix train')\n",
        "        acc_test = accuracy_fromcm(cm_test)\n",
        "        acc_val = accuracy_fromcm(cm_val)\n",
        "        acc_train = accuracy_fromcm(cm_train)\n",
        "        f1_test = f1score_fromcm(cm_test)\n",
        "        f1_val = f1score_fromcm(cm_val)\n",
        "        f1_train = f1score_fromcm(cm_train)\n",
        "\n",
        "    if debug:\n",
        "        if nextday_trading_price >= 0:\n",
        "            print('gain next-day strategy (test):', gain_test[-1])\n",
        "            print('gain next-day strategy (val):', gain_val[-1])\n",
        "            print('gain next-day strategy (train):', gain_train[-1])\n",
        "            print('optimal gain next-day strategy (test):', gain_test_optimal[-1])\n",
        "            print('optimal gain next-day strategy (val):', gain_val_optimal[-1])\n",
        "            print('optimal gain next-day strategy (train):', gain_train_optimal[-1])\n",
        "\n",
        "            print('accuracy test:', acc_test)\n",
        "            print('accuracy val:', acc_val)\n",
        "            print('accuracy train:', acc_train)\n",
        "            print('f1 test:', f1_test)\n",
        "            print('f1 val:', f1_val)\n",
        "            print('f1 train:', f1_train)\n",
        "    if logging:\n",
        "        if nextday_trading_price >= 0:\n",
        "            wandb.run.summary['gain_next-day_strategy_test'] = gain_test[-1]\n",
        "            wandb.run.summary['gain_next-day_strategy_val'] = gain_val[-1]\n",
        "            wandb.run.summary['gain_next-day_strategy_train'] = gain_train[-1]\n",
        "            wandb.run.summary['optimal_gain_next-day_strategy_test'] = gain_test_optimal[-1]\n",
        "            wandb.run.summary['optimal_gain_next-day_strategy_val'] = gain_val_optimal[-1]\n",
        "            wandb.run.summary['optimal_gain_next-day_strategy_train'] = gain_train_optimal[-1]\n",
        "\n",
        "            wandb.run.summary[\"accuracy_test\"] = acc_test\n",
        "            wandb.run.summary[\"accuracy_val\"] = acc_val\n",
        "            wandb.run.summary[\"accuracy_train\"] = acc_train\n",
        "            wandb.run.summary[\"f1_test\"] = f1_test\n",
        "            wandb.run.summary[\"f1_val\"] = f1_val\n",
        "            wandb.run.summary[\"f1_train\"] = f1_train\n",
        "            wandb.run.summary[\"conf_matrix_test\"] = cm_test\n",
        "            wandb.run.summary[\"conf_matrix_val\"] = cm_val\n",
        "            wandb.run.summary[\"conf_matrix_train\"] = cm_train\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58xQhytNDaOV"
      },
      "source": [
        "def run_experiment(dataset_type:str, dataset_path:str, window_size:int, scale_sections:int, batch_size:int, max_epochs:int, model_type:str, model_params:dict, \n",
        "                   val_size:int = 365, test_size:int = 365, wandb_project_name:str = None, wandb_entity:str = None, debug:bool = True, logging:bool = True, \n",
        "                   nextday_trading_price=0, plotsWithoutTeacher:bool=False, gpus:int = 0, cols_to_use=['Open'], scaler_type='minmax', eval_every_n_epochs=10, early_stopping=False,\n",
        "                   min_epochs=1, early_stopping_patience=10):\n",
        "    '''\n",
        "    function to run a complete experiment\n",
        "    '''\n",
        "    \n",
        "    dataset_train = getDataset(dataset_type, dataset_path, 'train', val_size=val_size, test_size=test_size, window_size=window_size, scale_sections=scale_sections, cols_to_use=cols_to_use, scaler_type=scaler_type)\n",
        "    dataset_val = getDataset(dataset_type, dataset_path, 'val', val_size=val_size, test_size=test_size, window_size=window_size, scaler=dataset_train.scaler, cols_to_use=cols_to_use, scaler_type=scaler_type)\n",
        "    dataset_test = getDataset(dataset_type, dataset_path, 'test', val_size=val_size, test_size=test_size, window_size=window_size, scaler=dataset_train.scaler, cols_to_use=cols_to_use, scaler_type=scaler_type)\n",
        "\n",
        "    loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=False)\n",
        "    loader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
        "    loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    if debug:\n",
        "        print('train set size:', len(dataset_train))\n",
        "        print('validation set size:', len(dataset_val))\n",
        "        print('test set size:', len(dataset_test))\n",
        "    \n",
        "    logger = None\n",
        "    if logging:\n",
        "        wandb.init(project=wandb_project_name, entity=wandb_entity)\n",
        "        dataset_train.log(wandb.config)\n",
        "        wandb.config.batch_size = batch_size\n",
        "        wandb.config.max_epochs = max_epochs\n",
        "        wandb.config.model_type = model_type\n",
        "        wandb.config.model_params = model_params\n",
        "        logger = WandbLogger(project=WANDB_PROJECT_NAME, log_model=True)\n",
        "    \n",
        "    model = getModel(dataset_train, model_type, model_params)\n",
        "\n",
        "    early_stop_callback = EarlyStopping(monitor='val_loss_epoch', min_delta=0.00, patience=early_stopping_patience, verbose=False, strict=False)\n",
        "    callbacks = []\n",
        "    if early_stopping:\n",
        "        callbacks = [early_stop_callback]\n",
        "    trainer = pl.Trainer(gpus=gpus, max_epochs = max_epochs, logger=logger, log_every_n_steps=30, \n",
        "                         check_val_every_n_epoch=eval_every_n_epochs, callbacks=callbacks, min_epochs=min_epochs) #, deterministic=True\n",
        "    if logger is not None:\n",
        "        logger.watch(model)\n",
        "    trainer.fit(model, loader_train, loader_val)\n",
        "\n",
        "    #validation\n",
        "    performValidation(trainer, model, dataset_train, dataset_val, dataset_test, loader_test, debug, logging, plotsWithoutTeacher, nextday_trading_price)\n",
        "\n",
        "    if logging:\n",
        "        wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xq22gz_RYV_"
      },
      "source": [
        "#Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2k5U9Mn5dbT"
      },
      "source": [
        "WANDB_PROJECT_NAME = 'StockPredictions'\n",
        "SEED = 42\n",
        "\n",
        "torch.set_default_dtype(torch.float64)\n",
        "pl.seed_everything(SEED)\n",
        "\n",
        "dsPath = '/content/drive/My Drive/DL/BTC-USD.csv'\n",
        "#dsPath = '/content/drive/My Drive/DL/GOOG.csv'\n",
        "#dsPath = '/content/drive/My Drive/DL/AAPL.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1NsYgxtzxHd"
      },
      "source": [
        "##MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_1S5O3l0G6Q"
      },
      "source": [
        "#============= MLP\n",
        "run_experiment('AutoregressiveDataset', dsPath, window_size = 25, scale_sections = 1, batch_size = 64, max_epochs = 400, model_type = 'MLP', model_params = {'hidden_size':200, 'loss_name':'mse', \n",
        "                'learning_rate':0.001, 'use_lr_sched':True, 'lr_decay_step': 150, 'lr_decay_gamma': 0.1}, \n",
        "                val_size = 365, test_size = 365, wandb_project_name = WANDB_PROJECT_NAME, wandb_entity = 'mirko222', gpus=1, cols_to_use=['Open'], logging=True)\n",
        "\n",
        "run_experiment('AutoregressiveDataset', dsPath, window_size = 25, scale_sections = 1, batch_size = 64, max_epochs = 400, model_type = 'MLP', model_params = {'hidden_size':200, 'loss_name':'mse+exp_trend4', \n",
        "                'learning_rate':0.001, 'use_lr_sched':True, 'lr_decay_step': 150, 'lr_decay_gamma': 0.1}, \n",
        "                val_size = 365, test_size = 365, wandb_project_name = WANDB_PROJECT_NAME, wandb_entity = 'mirko222', gpus=1, cols_to_use=['Open'], logging=True)\n",
        "\n",
        "run_experiment('AutoregressiveDistanceDataset', dsPath, window_size = 25, scale_sections = 1, batch_size = 64, max_epochs = 400, model_type = 'MLP', model_params = {'hidden_size':200, 'loss_name':'mse', \n",
        "                'learning_rate':0.001, 'use_lr_sched':True, 'lr_decay_step': 150, 'lr_decay_gamma': 0.1}, \n",
        "                val_size = 365, test_size = 365, wandb_project_name = WANDB_PROJECT_NAME, wandb_entity = 'mirko222', gpus=1, cols_to_use=['Open'], logging=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bXKKQ0DRA6W"
      },
      "source": [
        "##LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asD6kTF3Purn"
      },
      "source": [
        "#============= LSTM\n",
        "for i in range(3):\n",
        "    run_experiment('AutoregressiveDataset', dsPath, window_size = 25, scale_sections = 1, batch_size = 64, max_epochs = 300, model_type = 'StackedLSTM', model_params = {'hidden_size':100, 'num_layers':1, \n",
        "                    'feed_forward_depth':1, 'learning_rate':1e-3, 'use_lr_sched':True, 'lr_decay_step': 125, 'lr_decay_gamma': 0.1, 'loss_name':'mse', 'loss_lambda':0.1}, \n",
        "                    cols_to_use=['Open'], scaler_type='minmax', val_size = 365, test_size = 365, wandb_project_name = WANDB_PROJECT_NAME, wandb_entity = 'mirko222', gpus=1, logging=True)\n",
        "    \n",
        "    run_experiment('AutoregressiveDistanceDataset', dsPath, window_size = 25, scale_sections = 1, batch_size = 64, max_epochs = 300, model_type = 'StackedLSTM', model_params = {'hidden_size':100, 'num_layers':1, \n",
        "                    'feed_forward_depth':1, 'learning_rate':1e-3, 'use_lr_sched':True, 'lr_decay_step': 125, 'lr_decay_gamma': 0.1, 'loss_name':'mse', 'loss_lambda':0.1}, \n",
        "                    cols_to_use=['Open'], scaler_type='minmax', val_size = 365, test_size = 365, wandb_project_name = WANDB_PROJECT_NAME, wandb_entity = 'mirko222', gpus=1, logging=True)\n",
        "    \n",
        "    run_experiment('AutoregressiveDataset', dsPath, window_size = 25, scale_sections = 1, batch_size = 64, max_epochs = 300, model_type = 'StackedLSTM', model_params = {'hidden_size':100, 'num_layers':1, \n",
        "                    'feed_forward_depth':1, 'learning_rate':1e-3, 'use_lr_sched':True, 'lr_decay_step': 125, 'lr_decay_gamma': 0.1, 'loss_name':'mse+exp_trend4', 'loss_lambda': 0.1}, \n",
        "                    cols_to_use=['Open'], scaler_type='minmax', val_size = 365, test_size = 365, wandb_project_name = WANDB_PROJECT_NAME, wandb_entity = 'mirko222', gpus=1, logging=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlWDiA5PRDR2"
      },
      "source": [
        "##Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHaTuRQ8lfH_"
      },
      "source": [
        "for i in range(3):\n",
        "    run_experiment('AutoregressiveDataset', dsPath, window_size = 25, scale_sections = 1, batch_size = 64, max_epochs = 200, model_type = 'Transformer', model_params = {'embedding_size':128, \n",
        "                'dropout_prob':0.1, 'loss_name':'mse+exp_trend4', 'attention_heads': 2, 'num_encoder_blocks': 2, 'transformer_hidden_size': 2048, \n",
        "                'warmup_steps': 200, 'loss_lambda':1, 'use_lstm_for_embeddings':True, 'append_input_decoding': True, 'l2_penalty_lambda':0, 'use_amsgrad': False, 'use_lr_sched':True}, \n",
        "                cols_to_use=['Open'], scaler_type='minmax', val_size = 365, test_size = 365, wandb_project_name = WANDB_PROJECT_NAME, wandb_entity = 'mirko222', gpus=1, logging=True,\n",
        "                eval_every_n_epochs=1, early_stopping=False, min_epochs=150, early_stopping_patience=10)\n",
        "        \n",
        "    run_experiment('AutoregressiveDataset', dsPath, window_size = 25, scale_sections = 1, batch_size = 64, max_epochs = 200, model_type = 'Transformer', model_params = {'embedding_size':128, \n",
        "                'dropout_prob':0.1, 'loss_name':'mse', 'attention_heads': 2, 'num_encoder_blocks': 2, 'transformer_hidden_size': 2048, \n",
        "                'warmup_steps': 200, 'loss_lambda':1, 'use_lstm_for_embeddings':True, 'append_input_decoding': True, 'l2_penalty_lambda':0, 'use_amsgrad': False, 'use_lr_sched':True}, \n",
        "                cols_to_use=['Open'], scaler_type='minmax', val_size = 365, test_size = 365, wandb_project_name = WANDB_PROJECT_NAME, wandb_entity = 'mirko222', gpus=1, logging=True,\n",
        "                eval_every_n_epochs=1, early_stopping=False, min_epochs=150, early_stopping_patience=10)\n",
        "\n",
        "    run_experiment('AutoregressiveDistanceDataset', dsPath, window_size = 25, scale_sections = 1, batch_size = 64, max_epochs = 100, model_type = 'Transformer', model_params = {'embedding_size':128, \n",
        "                'dropout_prob':0.1, 'loss_name':'mse', 'attention_heads': 2, 'num_encoder_blocks': 2, 'transformer_hidden_size': 2048, \n",
        "                'warmup_steps': 200, 'loss_lambda':1, 'use_lstm_for_embeddings':True, 'append_input_decoding': True, 'l2_penalty_lambda':0, 'use_amsgrad': False, 'use_lr_sched':True}, \n",
        "                cols_to_use=['Open'], scaler_type='minmax', val_size = 365, test_size = 365, wandb_project_name = WANDB_PROJECT_NAME, wandb_entity = 'mirko222', gpus=1, logging=True,\n",
        "                eval_every_n_epochs=1, early_stopping=False, min_epochs=1, early_stopping_patience=10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}